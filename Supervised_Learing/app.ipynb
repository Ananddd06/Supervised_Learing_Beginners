{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPERVISED LEARING \n",
    "# Here we have the following Project Description\n",
    "* The data are MC generated (see below) to simulate registration of high energy gamma particles in a ground-based atmospheric Cherenkov gamma telescope using the imaging technique. Cherenkov gamma telescope observes high energy gamma rays, taking advantage of the radiation emitted by charged particles produced inside the electromagnetic showers initiated by the gammas, and developing in the atmosphere. This Cherenkov radiation (of visible to UV wavelengths) leaks through the atmosphere and gets recorded in the detector, allowing reconstruction of the shower parameters. The available information consists of pulses left by the incoming Cherenkov photons on the photomultiplier tubes, arranged in a plane, the camera. Depending on the energy of the primary gamma, a total of few hundreds to some 10000 Cherenkov photons get collected, in patterns (called the shower image), allowing to discriminate statistically those caused by primary gammas (signal) from the images of hadronic showers initiated by cosmic rays in the upper atmosphere (background).\n",
    "\n",
    "* Typically, the image of a shower after some pre-processing is an elongated cluster. Its long axis is oriented towards the camera center if the shower axis is parallel to the telescope's optical axis, i.e. if the telescope axis is directed towards a point source. A principal component analysis is performed in the camera plane, which results in a correlation axis and defines an ellipse. If the depositions were distributed as a bivariate Gaussian, this would be an equidensity ellipse. The characteristic parameters of this ellipse (often called Hillas parameters) are among the image parameters that can be used for discrimination. The energy depositions are typically asymmetric along the major axis, and this asymmetry can also be used in discrimination. There are, in addition, further discriminating characteristics, like the extent of the cluster in the image plane, or the total sum of depositions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature and Target Description\n",
    "\n",
    "## Variable Details\n",
    "\n",
    "| Variable Name | Role     | Type       | Description                                        | Units | Missing Values |\n",
    "|---------------|----------|------------|--------------------------------------------------|-------|----------------|\n",
    "| fLength       | Feature  | Continuous | major axis of ellipse                            | mm    | no             |\n",
    "| fWidth        | Feature  | Continuous | minor axis of ellipse                            | mm    | no             |\n",
    "| fSize         | Feature  | Continuous | 10-log of sum of content of all pixels           | #phot | no             |\n",
    "| fConc         | Feature  | Continuous | ratio of sum of two highest pixels over fSize    |       | no             |\n",
    "| fConc1        | Feature  | Continuous | ratio of highest pixel over fSize                |       | no             |\n",
    "| fAsym         | Feature  | Continuous | distance from highest pixel to center, projected onto major axis |       | no             |\n",
    "| fM3Long       | Feature  | Continuous | 3rd root of third moment along major axis        | mm    | no             |\n",
    "| fM3Trans      | Feature  | Continuous | 3rd root of third moment along minor axis        | mm    | no             |\n",
    "| fAlpha        | Feature  | Continuous | angle of major axis with vector to origin        | deg   | no             |\n",
    "| fDist         | Feature  | Continuous | distance from origin to center of ellipse        | mm    | no             |\n",
    "| class         | Target   | Binary     | gamma (signal), hadron (background)              |       | no             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fLength</th>\n",
       "      <th>fWidth</th>\n",
       "      <th>fSize</th>\n",
       "      <th>fConc</th>\n",
       "      <th>fConc1</th>\n",
       "      <th>fAsym</th>\n",
       "      <th>fM2Long</th>\n",
       "      <th>fM3Trans</th>\n",
       "      <th>fAlpha</th>\n",
       "      <th>fDist</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.7967</td>\n",
       "      <td>16.0021</td>\n",
       "      <td>2.6449</td>\n",
       "      <td>0.3918</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>27.7004</td>\n",
       "      <td>22.0110</td>\n",
       "      <td>-8.2027</td>\n",
       "      <td>40.0920</td>\n",
       "      <td>81.8828</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.6036</td>\n",
       "      <td>11.7235</td>\n",
       "      <td>2.5185</td>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>26.2722</td>\n",
       "      <td>23.8238</td>\n",
       "      <td>-9.9574</td>\n",
       "      <td>6.3609</td>\n",
       "      <td>205.2610</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.0520</td>\n",
       "      <td>136.0310</td>\n",
       "      <td>4.0612</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>116.7410</td>\n",
       "      <td>-64.8580</td>\n",
       "      <td>-45.2160</td>\n",
       "      <td>76.9600</td>\n",
       "      <td>256.7880</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.8172</td>\n",
       "      <td>9.5728</td>\n",
       "      <td>2.3385</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>27.2107</td>\n",
       "      <td>-6.4633</td>\n",
       "      <td>-7.1513</td>\n",
       "      <td>10.4490</td>\n",
       "      <td>116.7370</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.1362</td>\n",
       "      <td>30.9205</td>\n",
       "      <td>3.1611</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>-5.5277</td>\n",
       "      <td>28.5525</td>\n",
       "      <td>21.8393</td>\n",
       "      <td>4.6480</td>\n",
       "      <td>356.4620</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fLength    fWidth   fSize   fConc  fConc1     fAsym  fM2Long  fM3Trans  \\\n",
       "0   28.7967   16.0021  2.6449  0.3918  0.1982   27.7004  22.0110   -8.2027   \n",
       "1   31.6036   11.7235  2.5185  0.5303  0.3773   26.2722  23.8238   -9.9574   \n",
       "2  162.0520  136.0310  4.0612  0.0374  0.0187  116.7410 -64.8580  -45.2160   \n",
       "3   23.8172    9.5728  2.3385  0.6147  0.3922   27.2107  -6.4633   -7.1513   \n",
       "4   75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277  28.5525   21.8393   \n",
       "\n",
       "    fAlpha     fDist class  \n",
       "0  40.0920   81.8828     g  \n",
       "1   6.3609  205.2610     g  \n",
       "2  76.9600  256.7880     g  \n",
       "3  10.4490  116.7370     g  \n",
       "4   4.6480  356.4620     g  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need the name for the columns right\n",
    "cols = [\"fLength\" , \"fWidth\", \"fSize\",\"fConc\",\"fConc1\",\"fAsym\",\"fM2Long\" ,\"fM3Trans\",\"fAlpha\",\"fDist\",\"class\"]\n",
    "dataset = pd.read_csv(\"magic04.data\" , names=cols) # Here names = cols is we are naming the each column with the title \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fLength</th>\n",
       "      <th>fWidth</th>\n",
       "      <th>fSize</th>\n",
       "      <th>fConc</th>\n",
       "      <th>fConc1</th>\n",
       "      <th>fAsym</th>\n",
       "      <th>fM2Long</th>\n",
       "      <th>fM3Trans</th>\n",
       "      <th>fAlpha</th>\n",
       "      <th>fDist</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.7967</td>\n",
       "      <td>16.0021</td>\n",
       "      <td>2.6449</td>\n",
       "      <td>0.3918</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>27.7004</td>\n",
       "      <td>22.0110</td>\n",
       "      <td>-8.2027</td>\n",
       "      <td>40.0920</td>\n",
       "      <td>81.8828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.6036</td>\n",
       "      <td>11.7235</td>\n",
       "      <td>2.5185</td>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>26.2722</td>\n",
       "      <td>23.8238</td>\n",
       "      <td>-9.9574</td>\n",
       "      <td>6.3609</td>\n",
       "      <td>205.2610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.0520</td>\n",
       "      <td>136.0310</td>\n",
       "      <td>4.0612</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>116.7410</td>\n",
       "      <td>-64.8580</td>\n",
       "      <td>-45.2160</td>\n",
       "      <td>76.9600</td>\n",
       "      <td>256.7880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.8172</td>\n",
       "      <td>9.5728</td>\n",
       "      <td>2.3385</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>27.2107</td>\n",
       "      <td>-6.4633</td>\n",
       "      <td>-7.1513</td>\n",
       "      <td>10.4490</td>\n",
       "      <td>116.7370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.1362</td>\n",
       "      <td>30.9205</td>\n",
       "      <td>3.1611</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>-5.5277</td>\n",
       "      <td>28.5525</td>\n",
       "      <td>21.8393</td>\n",
       "      <td>4.6480</td>\n",
       "      <td>356.4620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fLength    fWidth   fSize   fConc  fConc1     fAsym  fM2Long  fM3Trans  \\\n",
       "0   28.7967   16.0021  2.6449  0.3918  0.1982   27.7004  22.0110   -8.2027   \n",
       "1   31.6036   11.7235  2.5185  0.5303  0.3773   26.2722  23.8238   -9.9574   \n",
       "2  162.0520  136.0310  4.0612  0.0374  0.0187  116.7410 -64.8580  -45.2160   \n",
       "3   23.8172    9.5728  2.3385  0.6147  0.3922   27.2107  -6.4633   -7.1513   \n",
       "4   75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277  28.5525   21.8393   \n",
       "\n",
       "    fAlpha     fDist  class  \n",
       "0  40.0920   81.8828      1  \n",
       "1   6.3609  205.2610      1  \n",
       "2  76.9600  256.7880      1  \n",
       "3  10.4490  116.7370      1  \n",
       "4   4.6480  356.4620      1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"class\"].unique()\n",
    "dataset[\"class\"] = (dataset[\"class\"] == \"g\").astype(int)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in cols[:-1]:\n",
    "    # Here for analysing the data we are using the matlpotlib library for the histogram represetation to analyse the dataset\n",
    "    plt.hist(dataset[dataset[\"class\"]==1][label] , color='blue' , label= 'gamma' , alpha = 0.7 , density=True) \n",
    "        #here am asking the dataset of the class 1 that is hedron to be in the blue color\n",
    "    plt.hist(dataset[dataset[\"class\"]==0][label] , color='red' , label = 'hedron' , alpha = 0.7 , density=True)\n",
    "     #here am asking the dataset of the class 0 that is gamma to be in the blue color\n",
    "    plt.title(label)\n",
    "    plt.xlabel(label)\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum() # This is for checking if your dataset has any missing value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.drop_duplicates() this is for removing the duplicates\n",
    "# print(dataset.duplicated().sum()) this is for finding the number of duplicates in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training, Validation, and Testing\n",
    "\n",
    "In the process of building and evaluating a machine learning model, the dataset is typically divided into three distinct subsets: **Training**, **Validation**, and **Testing**. These subsets serve different purposes to ensure the model is both accurate and generalizes well to unseen data. Below is an explanation of each subset and its purpose:\n",
    "\n",
    "## 1. **Training Data**\n",
    "- **Purpose**: The training data is used to **train** the machine learning model. During this phase, the model learns the underlying patterns, relationships, and features in the dataset.\n",
    "- **How it's used**: The model is optimized using the training data through methods like gradient descent. The model’s weights and biases are updated to minimize the loss function during this phase.\n",
    "- **Proportion of dataset**: The training set typically represents the largest portion of the data, around 70-80% of the total dataset.\n",
    "\n",
    "## 2. **Validation Data**\n",
    "- **Purpose**: The validation data is used to tune the model’s **hyperparameters** and evaluate its performance **during** training. It helps in preventing overfitting by providing an unbiased evaluation while the model is still being trained.\n",
    "- **How it's used**: After each training epoch, the model is evaluated on the validation set. This helps in adjusting hyperparameters like learning rate, regularization, and deciding when to stop training (early stopping).\n",
    "- **Proportion of dataset**: The validation set typically represents about 10-15% of the total dataset.\n",
    "\n",
    "## 3. **Test Data**\n",
    "- **Purpose**: The test data is used to evaluate the **final performance** of the model after it has been trained and validated. This dataset helps assess how well the model generalizes to unseen data.\n",
    "- **How it's used**: After the model has completed training and tuning, it is tested on this set to measure its performance in terms of accuracy, precision, recall, F1 score, or other relevant metrics.\n",
    "- **Proportion of dataset**: The test set typically makes up about 10-15% of the total dataset.\n",
    "\n",
    "## 4. **Dataset Splitting**\n",
    "To ensure the model is trained and evaluated properly, we split the dataset into three subsets: Training, Validation, and Test sets.\n",
    "\n",
    "### Example of Splitting the Dataset:\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'data' is a pandas DataFrame containing the dataset\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the test data into validation and test sets\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Now we have:\n",
    "# train_data: 80% of the original data\n",
    "# val_data: 10% of the original data\n",
    "# test_data: 10% of the original data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "train , validate , test = np.split(dataset.sample(frac=1),[int(0.6 * len(dataset)),int(0.8 * len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Dataset Function\n",
    "\n",
    "This repository contains a Python function `scale_dataset` that standardizes and preprocesses a dataset for use in machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "## Code Explanation\n",
    "\n",
    "### 1. Input Data Splitting**\n",
    "```python\n",
    "x = dataframe[dataframe.columns[:-1]].values\n",
    "y = dataframe[dataframe.columns[-1]].values\n",
    "```\n",
    "* x: The independent variables (features), extracted from all columns except the last one in the DataFrame.\n",
    "* y: The dependent variable (target), extracted from the last column in the DataFrame.\n",
    "\n",
    "### 2. Feature Standarization\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "```\n",
    "* scaler: An instance of `StandardScaler` from `sklearn` used to standardize features.\n",
    "* x: The standardized features, transformed to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "### 3. Combination  Feature\n",
    "```python\n",
    "data = np.hstack((x, np.reshape(y, (-1, 1))))\n",
    "```\n",
    "* data: A combined dataset that horizontally stacks the standardized features and the target variable.\n",
    "* np.reshape(y, (-1, 1)): Reshapes the target variable into a column vector.\n",
    "* np.hstack: Horizontally stacks the standardized features (`x`) with the target variable (`y`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_datset(dataframe , oversample = False):\n",
    "    x = dataframe[dataframe.columns[:-1]].values\n",
    "    y = dataframe[dataframe.columns[-1]].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    x = scaler.fit_transform(x)\n",
    "\n",
    "    if oversample:\n",
    "        ros = RandomOverSampler()\n",
    "        x , y  = ros.fit_resample(x,y)\n",
    "\n",
    "    data  = np.hstack((x, np.reshape(y , (-1,1))))\n",
    "\n",
    "    return data , x , y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7407\n",
      "3936\n"
     ]
    }
   ],
   "source": [
    "print(len(train[train[\"class\"] == 1]))  \n",
    "print(len(train[train[\"class\"] == 0]))\n",
    "\n",
    "#  The length of the Train model for the gamma and the hadron is not same as they having count as 7413 and 3930 so it will become biased as\n",
    "# we dont get the perfect accuracy so we need to sample it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train , x_train , y_train = scale_datset(train , oversample=True)\n",
    "validate , x_validate , y_validate = scale_datset(validate, oversample=False)\n",
    "test , x_test , y_test = scale_datset(test , oversample=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14814\n",
      "14814\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(y_train))   \n",
    "\n",
    "# After the oversampled we had the x and the y train  to be equal sampled so that we can able to train the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) Algorithm\n",
    "\n",
    "## Overview\n",
    "\n",
    "The **K-Nearest Neighbors (KNN)** algorithm is a simple, intuitive, and versatile machine learning algorithm used for both classification and regression tasks. It is a **non-parametric**, **lazy learning** algorithm that makes predictions based on the proximity of data points.\n",
    "\n",
    "### Key Features:\n",
    "- **Lazy Learning:** KNN doesn't learn an explicit model. Instead, it memorizes the training dataset and makes predictions at runtime by examining the nearest data points.\n",
    "- **Non-Parametric:** KNN doesn't assume any underlying data distribution.\n",
    "- **Instance-Based Learning:** KNN makes decisions based on the instances in the training data without creating a generalized model.\n",
    "\n",
    "## How KNN Works\n",
    "\n",
    "### Classification:\n",
    "1. **Choose the number of neighbors (k):**\n",
    "   - `k` is a hyperparameter that determines how many nearest neighbors to consider when making a prediction.\n",
    "   - If `k = 3`, for example, KNN will look at the 3 nearest neighbors and choose the most frequent class label among them.\n",
    "\n",
    "2. **Compute Distance:**\n",
    "   - The distance between a new data point (query point) and each point in the training dataset is computed. Common distance metrics include:\n",
    "     - **Euclidean Distance:** Most widely used, calculated as:\n",
    "       \\[\n",
    "       \\text{Distance} = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
    "       \\]\n",
    "     - **Manhattan Distance:** Alternative, computed as the sum of absolute differences:\n",
    "       \\[\n",
    "       \\text{Distance} = \\sum_{i=1}^{n} |x_i - y_i|\n",
    "       \\]\n",
    "     - **Minkowski Distance:** A generalization of both Euclidean and Manhattan distances.\n",
    "\n",
    "3. **Sort the Data:**\n",
    "   - After calculating distances, the algorithm sorts the training dataset by the distance from the query point.\n",
    "\n",
    "4. **Vote for Classification:**\n",
    "   - The `k` nearest neighbors' class labels are examined, and the most frequent label is returned as the prediction.\n",
    "\n",
    "### Regression:\n",
    "- In regression tasks, KNN predicts the value of the query point by averaging the values of the `k` nearest neighbors.\n",
    "\n",
    "## The Math Behind KNN\n",
    "\n",
    "### Distance Metrics:\n",
    "KNN uses various distance metrics to determine the proximity of data points:\n",
    "\n",
    "1. **Euclidean Distance:**\n",
    "   The most common measure of distance used in KNN.\n",
    "   \\[\n",
    "   \\text{Euclidean Distance} = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
    "   \\]\n",
    "   where `x_i` and `y_i` are the coordinates of the data points in the n-dimensional space.\n",
    "\n",
    "2. **Manhattan Distance:**\n",
    "   The sum of absolute differences between corresponding feature values.\n",
    "   \\[\n",
    "   \\text{Manhattan Distance} = \\sum_{i=1}^{n} |x_i - y_i|\n",
    "   \\]\n",
    "\n",
    "3. **Minkowski Distance:**\n",
    "   A generalization of both Euclidean and Manhattan distances. For `p = 2,` it's equivalent to Euclidean distance, and for `p = 1,` it is Manhattan distance.\n",
    "   \\[\n",
    "   \\text{Minkowski Distance} = \\left( \\sum_{i=1}^{n} |x_i - y_i|^p \\right)^{1/p}\n",
    "   \\]\n",
    "\n",
    "### Choosing k:\n",
    "- A small value of `k` can lead to a noisy model, while a large value can make the model too smooth, ignoring smaller patterns.\n",
    "- It's common to test various `k` values using cross-validation to determine the best `k`.\n",
    "\n",
    "## Why Use KNN?\n",
    "\n",
    "### Advantages:\n",
    "- **Simple and Intuitive:** The algorithm is easy to understand and implement.\n",
    "- **No Training Phase:** Since KNN is a lazy learner, it doesn't require a training phase, making it faster for certain use cases.\n",
    "- **Works Well with Small to Medium-Sized Datasets:** KNN works efficiently when the dataset is not too large, as the computational complexity increases with large datasets.\n",
    "\n",
    "### Disadvantages:\n",
    "- **Computationally Expensive:** For large datasets, calculating distances for every prediction can be slow.\n",
    "- **Memory-Intensive:** The algorithm needs to store the entire dataset in memory.\n",
    "- **Sensitive to Irrelevant Features:** KNN's performance can degrade if the data contains irrelevant or noisy features.\n",
    "- **Choosing k Can Be Challenging:** The optimal value of `k` may vary for different datasets and tasks.\n",
    "\n",
    "## Types of Problems KNN Can Solve\n",
    "\n",
    "### 1. **Classification Problems:**\n",
    "   - **Spam Detection:** Classifying emails as spam or not spam based on text features.\n",
    "   - **Image Classification:** Identifying objects in images by comparing pixel values to labeled data points.\n",
    "   - **Customer Segmentation:** Classifying customers based on purchasing behavior or demographics.\n",
    "\n",
    "### 2. **Regression Problems:**\n",
    "   - **Predicting House Prices:** Estimating house prices based on features like size, location, and number of rooms.\n",
    "   - **Stock Price Prediction:** Predicting stock prices based on historical data.\n",
    "\n",
    "### 3. **Anomaly Detection:**\n",
    "   - Identifying rare data points that don't follow the usual patterns (e.g., fraud detection).\n",
    "\n",
    "### 4. **Recommendation Systems:**\n",
    "   - **Collaborative Filtering:** Recommending items based on the preferences of similar users.\n",
    "\n",
    "## When to Use KNN\n",
    "\n",
    "- **When the dataset is small to medium-sized** and can fit into memory.\n",
    "- **When the decision boundary is highly non-linear**, as KNN can model complex decision surfaces.\n",
    "- **When the problem is based on proximity or similarity** between data points.\n",
    "\n",
    "## When Not to Use KNN\n",
    "\n",
    "- **Large datasets:** KNN requires a lot of memory and computational resources for large datasets because it calculates the distance for every data point at runtime.\n",
    "- **High-dimensional data (curse of dimensionality):** The performance of KNN can degrade significantly when dealing with high-dimensional data (i.e., datasets with many features).\n",
    "- **When the data is very sparse (few neighbors)** or when there is significant noise in the data.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "K-Nearest Neighbors is a simple yet powerful algorithm that is widely used for both classification and regression tasks. Despite its simplicity, its performance heavily depends on choosing the right `k` and distance metric, and its computation cost can be a concern for large datasets. KNN is most effective when the dataset is small and the relationships between data points are based on proximity.\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to experiment with different distance metrics and values of `k` to tune the KNN algorithm for your specific use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "Knn_model.fit(x_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = Knn_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.73      0.72      1316\n",
      "           1       0.85      0.85      0.85      2465\n",
      "\n",
      "    accuracy                           0.81      3781\n",
      "   macro avg       0.79      0.79      0.79      3781\n",
      "weighted avg       0.81      0.81      0.81      3781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
